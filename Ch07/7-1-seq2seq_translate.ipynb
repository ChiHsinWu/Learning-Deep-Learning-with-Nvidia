{"cells":[{"cell_type":"markdown","source":["程式7-1"],"metadata":{"id":"9CoDaYRHtQ1T"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","\n","# 宣告輸入層\n","inputs = Input(shape=(10,))\n","\n","# 宣告各神經層\n","layer1 = Dense(64, activation='relu')\n","layer2 = Dense(64, activation='relu')\n","\n","# 連接輸入層與各神經層\n","layer1_outputs = layer1(inputs)\n","layer2_outputs = layer2(layer1_outputs)\n","\n","# 建立模型\n","model = Model(inputs=inputs, outputs=layer2_outputs)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Bdb8A7ltU45","outputId":"d7daa1b9-70b6-4f9d-faf7-0433cdf60d93","executionInfo":{"status":"ok","timestamp":1699164359752,"user_tz":-480,"elapsed":333,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 10)]              0         \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                704       \n","                                                                 \n"," dense_5 (Dense)             (None, 64)                4160      \n","                                                                 \n","=================================================================\n","Total params: 4864 (19.00 KB)\n","Trainable params: 4864 (19.00 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["程式 7-2"],"metadata":{"id":"oXYqvcnYMOq_"}},{"cell_type":"code","source":["# 宣告兩組輸入\n","inputs = Input(shape=(10,))\n","bypass_inputs = Input(shape=(5,))\n","\n","# 宣告各神經層\n","layer1 = Dense(64, activation='relu')\n","concat_layer = Concatenate()\n","layer2 = Dense(64, activation='relu')\n","\n","# 連接輸入層與各神經層\n","layer1_outputs = layer1(inputs)\n","layer2_inputs = concat_layer([layer1_outputs, bypass_inputs])\n","layer2_outputs = layer2(layer2_inputs)\n","\n","# 建構模型\n","model = Model(inputs=[inputs, bypass_inputs],outputs=layer2_outputs)\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRaxVVoMtk8R","outputId":"3de57bd5-53e6-417c-99aa-77cfe7f5d171"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, 10)]         0           []                               \n","                                                                                                  \n"," dense_2 (Dense)                (None, 64)           704         ['input_2[0][0]']                \n","                                                                                                  \n"," input_3 (InputLayer)           [(None, 5)]          0           []                               \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 69)           0           ['dense_2[0][0]',                \n","                                                                  'input_3[0][0]']                \n","                                                                                                  \n"," dense_3 (Dense)                (None, 64)           4480        ['concatenate[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,184\n","Trainable params: 5,184\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["程式7-3"],"metadata":{"id":"EQY74fcbRuwP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VK1ObZX6kxed"},"outputs":[],"source":["import numpy as np\n","import random\n","import os\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.text \\\n","    import text_to_word_sequence\n","from tensorflow.keras.preprocessing.sequence \\\n","    import pad_sequences\n","from tensorflow.keras.layers import Concatenate\n","import tensorflow as tf\n","import logging\n","tf.get_logger().setLevel(logging.ERROR)\n"]},{"cell_type":"markdown","source":["程式7-4"],"metadata":{"id":"KtKfHuEqF0KC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d7m6nCskxef"},"outputs":[],"source":["# 定義各種超參數以及常數\n","EPOCHS = 20\n","BATCH_SIZE = 128\n","MAX_WORDS = 10000\n","READ_LINES = 60000\n","LAYER_SIZE = 256\n","EMBEDDING_WIDTH = 128\n","TEST_PERCENT = 0.2\n","SAMPLE_SIZE = 20\n","OOV_WORD = 'UNK'\n","PAD_INDEX = 0\n","OOV_INDEX = 1\n","START_INDEX = MAX_WORDS - 2\n","STOP_INDEX = MAX_WORDS - 1\n","MAX_LENGTH = 60\n","SRC_DEST_FILE_NAME = '/content/fra.txt'\n","#SRC_DEST_FILE_NAME = path_head+'data/fra.txt'\n"]},{"cell_type":"markdown","metadata":{"id":"RSAlmknJkxef"},"source":["程式 7-5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEP4AKJbkxeg"},"outputs":[],"source":["# 定義一個讀取檔案的函式\n","def read_file_combined(file_name, max_len):\n","    file = open(file_name, 'r', encoding='utf-8')\n","    src_word_sequences = []\n","    dest_word_sequences = []\n","    for i, line in enumerate(file):\n","        if i == READ_LINES:\n","            break\n","        pair = line.split('\\t')\n","        word_sequence = text_to_word_sequence(pair[1])\n","        src_word_sequence = word_sequence[0:max_len]\n","        src_word_sequences.append(src_word_sequence)\n","        word_sequence = text_to_word_sequence(pair[0])\n","        dest_word_sequence = word_sequence[0:max_len]\n","        dest_word_sequences.append(dest_word_sequence)\n","    file.close()\n","    return src_word_sequences, dest_word_sequences\n"]},{"cell_type":"markdown","source":["程式7-6"],"metadata":{"id":"-jBEwaCaS0Ho"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gTJbbKx5kxeh"},"outputs":[],"source":["# 單字與編號序列互換函式\n","\n","#單字轉編號\n","def tokenize(sequences):\n","     # \"MAX_WORDS-2\"是為了保留兩編號\n","     # 給 START、STOP 標記\n","    tokenizer = Tokenizer(num_words=MAX_WORDS-2,\n","                          oov_token=OOV_WORD)\n","    tokenizer.fit_on_texts(sequences)\n","    token_sequences = tokenizer.texts_to_sequences(sequences)\n","    return tokenizer, token_sequences\n","\n","#編號轉單字\n","def tokens_to_words(tokenizer, seq):\n","    word_seq = []\n","    for index in seq:\n","        if index == PAD_INDEX:\n","            word_seq.append('PAD')\n","        elif index == OOV_INDEX:\n","            word_seq.append(OOV_WORD)\n","        elif index == START_INDEX:\n","            word_seq.append('START')\n","        elif index == STOP_INDEX:\n","            word_seq.append('STOP')\n","        else:\n","            word_seq.append(tokenizer.sequences_to_texts(\n","                [[index]])[0])\n","    print(word_seq)\n"]},{"cell_type":"markdown","metadata":{"id":"XcbO6FGjkxei"},"source":["程式7-7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9fQ5-kikxei"},"outputs":[],"source":["# 讀取檔案並斷字\n","src_seq, dest_seq = read_file_combined(SRC_DEST_FILE_NAME,\n","                    MAX_LENGTH)\n","src_tokenizer, src_token_seq = tokenize(src_seq)\n","dest_tokenizer, dest_token_seq = tokenize(dest_seq)\n"]},{"cell_type":"code","source":["print(src_seq[9999])\n","print(dest_seq[9999])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTJsViD4cUvM","executionInfo":{"status":"ok","timestamp":1699416962760,"user_tz":-480,"elapsed":304,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}},"outputId":"e7f857a0-7daf-4fb2-d544-b6bb91b82cf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['quel', 'fiasco']\n","['what', 'a', 'fiasco']\n"]}]},{"cell_type":"code","source":["dest_token_seq[9999]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qNnKdlkZiMrd","executionInfo":{"status":"ok","timestamp":1699336252605,"user_tz":-480,"elapsed":464,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}},"outputId":"73078f38-862d-4203-826c-978af5bd4eb4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[35, 5, 3807]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["程式7-8"],"metadata":{"id":"QEnG39nG7DkR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWEEw4tqkxej"},"outputs":[],"source":["# 準備訓練集 (測試集會從訓練集切割一部分出來)\n","\n","dest_target_token_seq = [x + [STOP_INDEX] for x in dest_token_seq]\n","dest_input_token_seq = [[START_INDEX] + x for x in\n","                        dest_target_token_seq]\n","src_input_data = pad_sequences(src_token_seq)\n","dest_input_data = pad_sequences(dest_input_token_seq,\n","                                padding='post')\n","dest_target_data = pad_sequences(\n","    dest_target_token_seq, padding='post', maxlen\n","    = len(dest_input_data[0]))\n"]},{"cell_type":"code","source":["print(src_input_data[9999])\n","print(dest_input_data[9999])\n","print(dest_target_data[9999])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4JS-hk3W6bF","executionInfo":{"status":"ok","timestamp":1699337191138,"user_tz":-480,"elapsed":971,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}},"outputId":"702f9656-c908-4df7-c19e-43939f5b3d8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[   0    0    0    0    0    0    0    0    0    0    0    0  136 6226]\n","[9998   35    5 3807 9999    0    0    0    0]\n","[  35    5 3807 9999    0    0    0    0    0]\n"]}]},{"cell_type":"code","source":["dest_target_token_seq[9999]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HYRZliS5uT2","executionInfo":{"status":"ok","timestamp":1699336309226,"user_tz":-480,"elapsed":387,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}},"outputId":"d3ef962c-838e-4cf9-afb4-588ffd859eb5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[35, 5, 3807, 9999]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["程式7-9"],"metadata":{"id":"m1HbLRnSe94t"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgxKf95Ukxek"},"outputs":[],"source":["# 將資料集拆成訓練集與測試集兩塊\n","rows = len(src_input_data[:,0])\n","all_indices = list(range(rows))\n","test_rows = int(rows * TEST_PERCENT)\n","test_indices = random.sample(all_indices, test_rows)\n","train_indices = [x for x in all_indices if x not in test_indices]\n","\n","train_src_input_data = src_input_data[train_indices]\n","train_dest_input_data = dest_input_data[train_indices]\n","train_dest_target_data = dest_target_data[train_indices]\n","\n","test_src_input_data = src_input_data[test_indices]\n","test_dest_input_data = dest_input_data[test_indices]\n","test_dest_target_data = dest_target_data[test_indices]\n","\n","#從測試集隨機抽出 20 筆 (SAMPLE_SIZE) 樣本 / 正解\n","test_indices = list(range(test_rows))\n","sample_indices = random.sample(test_indices, SAMPLE_SIZE)\n","sample_input_data = test_src_input_data[sample_indices]\n","sample_target_data = test_dest_target_data[sample_indices]\n"]},{"cell_type":"markdown","metadata":{"id":"_blD-fg1kxek"},"source":["程式7-10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fp3OL5lhkxek","outputId":"44943773-80fd-4a20-f65b-f3696cbe23f3","executionInfo":{"status":"ok","timestamp":1699416982569,"user_tz":-480,"elapsed":2500,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 128)         1280000   \n","                                                                 \n"," lstm (LSTM)                 [(None, None, 256),       394240    \n","                              (None, 256),                       \n","                              (None, 256)]                       \n","                                                                 \n"," lstm_1 (LSTM)               [(None, 256),             525312    \n","                              (None, 256),                       \n","                              (None, 256)]                       \n","                                                                 \n","=================================================================\n","Total params: 2199552 (8.39 MB)\n","Trainable params: 2199552 (8.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# 建構編碼器模型\n","# 輸入資料是原句 (法語)序列\n","enc_embedding_input = Input(shape=(None, ))\n","\n","# 建立編碼器各層\n","enc_embedding_layer = Embedding(\n","    output_dim=EMBEDDING_WIDTH, input_dim\n","    = MAX_WORDS, mask_zero=True)\n","enc_layer1 = LSTM(LAYER_SIZE, return_state=True,\n","                  return_sequences=True)\n","enc_layer2 = LSTM(LAYER_SIZE, return_state=True)\n","\n","# 建立編碼器各層\n","# 最末層輸出會被捨棄，僅保留最終內部狀態c 與 h 給解碼器\n","enc_embedding_layer_outputs = \\\n","    enc_embedding_layer(enc_embedding_input)\n","enc_layer1_outputs, enc_layer1_state_h, enc_layer1_state_c = \\\n","    enc_layer1(enc_embedding_layer_outputs)\n","_, enc_layer2_state_h, enc_layer2_state_c = \\\n","    enc_layer2(enc_layer1_outputs)\n","\n","# 建構模型\n","enc_model = Model(enc_embedding_input,\n","                  [enc_layer1_state_h, enc_layer1_state_c,\n","                   enc_layer2_state_h, enc_layer2_state_c])\n","enc_model.summary()\n"]},{"cell_type":"markdown","source":["程式7-11"],"metadata":{"id":"b7fLJAt2-zlK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gplzkreGkxel","outputId":"e72fd34a-312d-43fc-8249-9714d0de68eb","executionInfo":{"status":"ok","timestamp":1699416991676,"user_tz":-480,"elapsed":2663,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_6 (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," embedding_1 (Embedding)     (None, None, 128)            1280000   ['input_6[0][0]']             \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 256)]                0         []                            \n","                                                                                                  \n"," input_3 (InputLayer)        [(None, 256)]                0         []                            \n","                                                                                                  \n"," lstm_2 (LSTM)               [(None, None, 256),          394240    ['embedding_1[0][0]',         \n","                              (None, 256),                           'input_2[0][0]',             \n","                              (None, 256)]                           'input_3[0][0]']             \n","                                                                                                  \n"," input_4 (InputLayer)        [(None, 256)]                0         []                            \n","                                                                                                  \n"," input_5 (InputLayer)        [(None, 256)]                0         []                            \n","                                                                                                  \n"," lstm_3 (LSTM)               [(None, None, 256),          525312    ['lstm_2[0][0]',              \n","                              (None, 256),                           'input_4[0][0]',             \n","                              (None, 256)]                           'input_5[0][0]']             \n","                                                                                                  \n"," dense (Dense)               (None, None, 10000)          2570000   ['lstm_3[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 4769552 (18.19 MB)\n","Trainable params: 4769552 (18.19 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["# 建構解碼器模型\n","# 輸入資料為譯句序列\n","# 與編碼器傳來的 thought vector\n","dec_layer1_state_input_h = Input(shape=(LAYER_SIZE,))\n","dec_layer1_state_input_c = Input(shape=(LAYER_SIZE,))\n","dec_layer2_state_input_h = Input(shape=(LAYER_SIZE,))\n","dec_layer2_state_input_c = Input(shape=(LAYER_SIZE,))\n","dec_embedding_input = Input(shape=(None, ))\n","\n","# 建立解碼器各層\n","dec_embedding_layer = Embedding(output_dim=EMBEDDING_WIDTH,\n","                                input_dim=MAX_WORDS,\n","                                mask_zero=True)\n","dec_layer1 = LSTM(LAYER_SIZE, return_state = True,\n","                  return_sequences=True)\n","dec_layer2 = LSTM(LAYER_SIZE, return_state = True,\n","                  return_sequences=True)\n","dec_layer3 = Dense(MAX_WORDS, activation='softmax')\n","\n","# 連接解碼器各層\n","dec_embedding_layer_outputs = dec_embedding_layer(\n","    dec_embedding_input)\n","dec_layer1_outputs, dec_layer1_state_h, dec_layer1_state_c = \\\n","    dec_layer1(dec_embedding_layer_outputs,\n","    initial_state=[dec_layer1_state_input_h,\n","                   dec_layer1_state_input_c])\n","dec_layer2_outputs, dec_layer2_state_h, dec_layer2_state_c = \\\n","    dec_layer2(dec_layer1_outputs,\n","    initial_state=[dec_layer2_state_input_h,\n","                   dec_layer2_state_input_c])\n","dec_layer3_outputs = dec_layer3(dec_layer2_outputs)\n","\n","# 建構模型\n","dec_model = Model([dec_embedding_input,\n","                   dec_layer1_state_input_h,\n","                   dec_layer1_state_input_c,\n","                   dec_layer2_state_input_h,\n","                   dec_layer2_state_input_c],\n","                  [dec_layer3_outputs, dec_layer1_state_h,\n","                   dec_layer1_state_c, dec_layer2_state_h,\n","                   dec_layer2_state_c])\n","dec_model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"iYUPU_b6kxel"},"source":["程式7-12"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qj7O8ZDYkxel","outputId":"17db009d-aecf-4e5b-cc2d-048c8c6a8058","executionInfo":{"status":"ok","timestamp":1699417002162,"user_tz":-480,"elapsed":4979,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_7 (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," input_8 (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," model (Functional)          [(None, 256),                2199552   ['input_7[0][0]']             \n","                              (None, 256),                                                        \n","                              (None, 256),                                                        \n","                              (None, 256)]                                                        \n","                                                                                                  \n"," model_1 (Functional)        [(None, None, 10000),        4769552   ['input_8[0][0]',             \n","                              (None, 256),                           'model[0][0]',               \n","                              (None, 256),                           'model[0][1]',               \n","                              (None, 256),                           'model[0][2]',               \n","                              (None, 256)]                           'model[0][3]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 6969104 (26.59 MB)\n","Trainable params: 6969104 (26.59 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["# 建構並編譯整個訓練模型\n","\n","train_enc_embedding_input = Input(shape=(None, ))\n","train_dec_embedding_input = Input(shape=(None, ))\n","intermediate_state = enc_model(train_enc_embedding_input)\n","train_dec_output, _, _, _, _ = dec_model(\n","    [train_dec_embedding_input] +\n","    intermediate_state)\n","training_model = Model([train_enc_embedding_input,\n","                        train_dec_embedding_input],\n","                        train_dec_output)\n","optimizer = RMSprop(lr=0.01)\n","training_model.compile(loss='sparse_categorical_crossentropy',\n","                       optimizer=optimizer, metrics =['accuracy'])\n","training_model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"WRzlnSehkxem"},"source":["程式 7-13"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEZLZIVbkxem","outputId":"469e954a-e46c-4011-9efe-48a2d5dded5b","executionInfo":{"status":"ok","timestamp":1699430905591,"user_tz":-480,"elapsed":1013460,"user":{"displayName":"Tristan Chang","userId":"01396821554749831700"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["step:  0\n","375/375 [==============================] - 490s 1s/step - loss: 1.6092 - accuracy: 0.6966 - val_loss: 1.8150 - val_accuracy: 0.6745\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'pas', 'un', 'son', 'ne', 'pouvait', 'être', 'entendu']\n","['not', 'a', 'sound', 'was', 'heard', 'STOP', 'PAD', 'PAD', 'PAD']\n","['no', 'only', 'hurt', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'nous', 'voici\\u202f']\n","['here', 'we', 'are', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"we'll\", 'get', 'up', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'devez', 'être', 'prudentes']\n","['you', 'must', 'be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['you', 'must', 'be', 'right', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'nous', 'mangerons', 'ton', 'pain']\n","[\"we'll\", 'eat', 'your', 'bread', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"we'll\", 'get', 'your', 'key', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tu', 'es', 'objectif']\n","[\"you're\", 'objective', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"you're\", 'crafty', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ne', 'pouvez', 'vous', 'nous', 'aider']\n","[\"can't\", 'you', 'help', 'us', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"can't\", 'you', 'help', 'us', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'où', 'sont', 'mes', 'UNK']\n","['where', 'are', 'my', 'glasses', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['where', 'are', 'my', 'friends', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'adore\", 'votre', 'jardin']\n","['i', 'love', 'your', 'garden', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['i', 'love', 'your', 'car', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'où', 'est', 'la', 'UNK']\n","[\"where's\", 'the', 'remote', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"where's\", 'the', 'cat', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'comment', 'va', 'votre', 'famille']\n","['how', 'is', 'your', 'family', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"how's\", 'your', 'wife', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'me', 'suis', 'fait', 'vacciner', 'contre', 'la', 'grippe']\n","['i', 'had', 'a', 'flu', 'shot', 'STOP', 'PAD', 'PAD', 'PAD']\n","['i', 'got', 'a', 'new', 'home', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tu', 'nous', \"l'as\", 'donné']\n","['you', 'gave', 'it', 'to', 'us', 'STOP', 'PAD', 'PAD', 'PAD']\n","['you', 'must', 'this', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'sais', 'que', 'tom', 'est', 'sévère']\n","['i', 'know', 'tom', 'is', 'tough', 'STOP', 'PAD', 'PAD', 'PAD']\n","['i', 'know', 'tom', 'is', 'rich', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'montre', 'moi', 'comment', 'ça', 'fonctionne']\n","['show', 'me', 'how', 'it', 'works', 'STOP', 'PAD', 'PAD', 'PAD']\n","['did', 'you', 'like', 'this', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'mauvais']\n","[\"that's\", 'not', 'bad', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"it's\", 'not', 'funny', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'suffisant']\n","[\"that's\", 'enough', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['this', 'is', 'enough', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'UNK', \"l'eau\", \"s'il\", 'te', 'plaît']\n","['please', 'heat', 'the', 'water', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['please', 'turn', 'the', 'water', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', \"m'ont\", 'enlevé']\n","['they', 'kidnapped', 'me', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['they', 'love', 'me', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'y', 'a', 'du', 'brouillard']\n","['it', 'is', 'foggy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"there's\", 'a', 'best', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'fiable']\n","[\"i'm\", 'dependable', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"i'm\", 'exhausted', 'STOP']\n","\n","\n","\n","step:  1\n","375/375 [==============================] - 489s 1s/step - loss: 1.5439 - accuracy: 0.7069 - val_loss: 1.7760 - val_accuracy: 0.6826\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'pas', 'un', 'son', 'ne', 'pouvait', 'être', 'entendu']\n","['not', 'a', 'sound', 'was', 'heard', 'STOP', 'PAD', 'PAD', 'PAD']\n","['no', 'only', 'know', 'it', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'nous', 'voici\\u202f']\n","['here', 'we', 'are', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"we'll\", 'get', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'vous', 'devez', 'être', 'prudentes']\n","['you', 'must', 'be', 'careful', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['you', 'must', 'be', 'cautious', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'nous', 'mangerons', 'ton', 'pain']\n","[\"we'll\", 'eat', 'your', 'bread', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['we', 'get', 'your', 'wine', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tu', 'es', 'objectif']\n","[\"you're\", 'objective', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"you're\", 'innocent', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ne', 'pouvez', 'vous', 'nous', 'aider']\n","[\"can't\", 'you', 'help', 'us', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['you', \"can't\", 'help', 'us', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'où', 'sont', 'mes', 'UNK']\n","['where', 'are', 'my', 'glasses', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['where', 'are', 'my', 'friends', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"j'adore\", 'votre', 'jardin']\n","['i', 'love', 'your', 'garden', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['i', 'love', 'your', 'room', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'où', 'est', 'la', 'UNK']\n","[\"where's\", 'the', 'remote', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"where's\", 'the', 'cat', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'comment', 'va', 'votre', 'famille']\n","['how', 'is', 'your', 'family', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"how's\", 'your', 'name', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'me', 'suis', 'fait', 'vacciner', 'contre', 'la', 'grippe']\n","['i', 'had', 'a', 'flu', 'shot', 'STOP', 'PAD', 'PAD', 'PAD']\n","['i', 'got', 'off', 'the', 'life', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'tu', 'nous', \"l'as\", 'donné']\n","['you', 'gave', 'it', 'to', 'us', 'STOP', 'PAD', 'PAD', 'PAD']\n","['you', 'should', 'it', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'sais', 'que', 'tom', 'est', 'sévère']\n","['i', 'know', 'tom', 'is', 'tough', 'STOP', 'PAD', 'PAD', 'PAD']\n","['i', 'know', 'tom', 'is', 'rich', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'montre', 'moi', 'comment', 'ça', 'fonctionne']\n","['show', 'me', 'how', 'it', 'works', 'STOP', 'PAD', 'PAD', 'PAD']\n","['show', 'me', 'that', 'job', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'ce', \"n'est\", 'pas', 'mauvais']\n","[\"that's\", 'not', 'bad', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"it's\", 'not', 'good', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', \"c'est\", 'suffisant']\n","[\"that's\", 'enough', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['this', 'is', 'enough', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'UNK', \"l'eau\", \"s'il\", 'te', 'plaît']\n","['please', 'heat', 'the', 'water', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD']\n","['please', 'water', 'water', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'elles', \"m'ont\", 'enlevé']\n","['they', 'kidnapped', 'me', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","['they', 'love', 'me', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'il', 'y', 'a', 'du', 'brouillard']\n","['it', 'is', 'foggy', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"there's\", 'a', 'problem', 'STOP']\n","\n","\n","\n","['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'je', 'suis', 'fiable']\n","[\"i'm\", 'dependable', 'STOP', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n","[\"i'm\", 'confused', 'STOP']\n","\n","\n","\n"]}],"source":["# 用雙層 for 迴圈交替進行訓練與測試工作\n","\n","for i in range(EPOCHS):\n","    print('Epoch: ' , i)\n","\n","    # 模型訓練一個週期\n","    history = training_model.fit(\n","        [train_src_input_data, train_dest_input_data],\n","        train_dest_target_data, validation_data=(\n","            [test_src_input_data, test_dest_input_data],\n","            test_dest_target_data), batch_size=BATCH_SIZE,\n","        epochs=1)\n","\n","    # 將事先挑出的測試樣本送入模型，生成其譯句\n","    for (test_input, test_target) in zip(sample_input_data,\n","                                         sample_target_data):\n","        # 將一原句輸入編碼器\n","        x = np.reshape(test_input, (1, -1))\n","        last_states = enc_model.predict(\n","            x, verbose=0)\n","\n","        # 將最終內部狀態 (即 thought vector) 與 START_INDEX 一併輸入解碼器\n","        prev_word_index = START_INDEX\n","        produced_string = ''\n","        pred_seq = []\n","        for j in range(MAX_LENGTH):\n","            x = np.reshape(np.array(prev_word_index), (1, 1))\n","\n","            # 生成單字、記錄此時內部狀態\n","            preds, dec_layer1_state_h, dec_layer1_state_c, \\\n","                dec_layer2_state_h, dec_layer2_state_c = \\\n","                    dec_model.predict(\n","                        [x] + last_states, verbose=0)\n","            last_states = [dec_layer1_state_h,\n","                           dec_layer1_state_c,\n","                           dec_layer2_state_h,\n","                           dec_layer2_state_c]\n","\n","            # 挑出可能性最高的單字\n","            prev_word_index = np.asarray(preds[0][0]).argmax()\n","            pred_seq.append(prev_word_index)\n","            if prev_word_index == STOP_INDEX:\n","                break\n","        tokens_to_words(src_tokenizer, test_input)\n","        tokens_to_words(dest_tokenizer, test_target)\n","        tokens_to_words(dest_tokenizer, pred_seq)\n","        print('\\n\\n')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"provenance":[],"gpuType":"T4"}},"nbformat":4,"nbformat_minor":0}